{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "N_ITERATIONS = 100000\n",
    "NUM_FEATURES = 7\n",
    "NUM_TRAINING_DAYS = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filepath, is_testing=False):\n",
    "    # Read the CSV file \n",
    "    if is_testing:\n",
    "        data = pd.read_csv(filepath, header=None)\n",
    "        data.columns = ['Date', 'ItemName', '0', '1', '2', '3', '4', '5', '6', '7', '8']\n",
    "    else:\n",
    "        data = pd.read_csv(filepath)\n",
    "        data.drop(['Location'], axis=1, inplace=True)\n",
    "\n",
    "    # Replace non-numeric values with NaN\n",
    "    non_numeric = {'#': np.nan, '*': np.nan, 'x': np.nan, 'A': np.nan}\n",
    "    data.replace(non_numeric, inplace=True)\n",
    "\n",
    "    # Convert the columns to numeric, except 'Location', 'Date', and 'ItemName'\n",
    "    numeric_columns = data.columns.difference(['Location', 'Date', 'ItemName'])\n",
    "    data[numeric_columns] = data[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Fill NaN values with the median of their respective columns\n",
    "    data[numeric_columns] = data[numeric_columns].fillna(0)\n",
    "\n",
    "    \n",
    "\n",
    "    # Strip the 'ItemName' column\n",
    "    data['ItemName'] = data['ItemName'].str.strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "# Use the function to load and preprocess your data\n",
    "data = load_and_preprocess_data('train.csv')\n",
    "test_data = load_and_preprocess_data('test.csv', is_testing=True)\n",
    "print(data.head())\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (240, 162)\n",
      "Targets shape: (240,)\n",
      "Test Features shape: (240, 162)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_features_targets(dataframe, is_train=True):\n",
    "    # Initialize lists to store features and targets\n",
    "    features_list = []\n",
    "    target_list = []\n",
    "\n",
    "    # Unique dates in the DataFrame\n",
    "    unique_dates = dataframe['Date'].unique()[:NUM_TRAINING_DAYS]\n",
    "\n",
    "    # Loop through each date to extract features and target\n",
    "    for date in unique_dates:\n",
    "        # Filter the DataFrame for the current date\n",
    "        daily_data = dataframe[dataframe['Date'] == date]\n",
    "\n",
    "        # Ensure that the data is sorted by ItemName to maintain consistent feature order\n",
    "        daily_data = daily_data.sort_values('ItemName')\n",
    "\n",
    "        # Extract the feature data for hours 0-9 for all elements\n",
    "        daily_features = daily_data.iloc[:, 2:11].values.flatten()  # Assuming 3rd column is hour 0\n",
    "\n",
    "        # Add the extracted features to the features list\n",
    "        features_list.append(daily_features)\n",
    "\n",
    "        if not is_train:\n",
    "            continue\n",
    "\n",
    "        # Extract the target data (PM2.5 at 10 AM)\n",
    "        pm25_data = daily_data[daily_data['ItemName'] == 'PM2.5']\n",
    "        pm25_at_10am = pm25_data.iloc[0, 11] if not pm25_data.empty else np.nan \n",
    "\n",
    "        # Add the target value to the target list\n",
    "        target_list.append(pm25_at_10am)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    features_array = np.array(features_list)\n",
    "    target_array = np.array(target_list)\n",
    "\n",
    "    return features_array, target_array\n",
    "\n",
    "\n",
    "# Now you can call the function with your DataFrame\n",
    "features, targets = extract_features_targets(data)\n",
    "test_features, _ = extract_features_targets(test_data, is_train=False)\n",
    "\n",
    "# Print shapes to confirm dimensions\n",
    "print('Features shape:', features.shape)\n",
    "print('Targets shape:', targets.shape)\n",
    "print('Test Features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 162) (240,)\n",
      "(240, 162)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(features)\n",
    "X = X.astype(float)\n",
    "y = np.array(targets)\n",
    "y = y.astype(float)\n",
    "\n",
    "X_test = np.array(test_features)\n",
    "X_test = X_test.astype(float)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features shape (training): (240, 63)\n",
      "Selected features shape (testing): (240, 63)\n"
     ]
    }
   ],
   "source": [
    "X_data_len = X.shape[0]\n",
    "X_test_data_len = X_test.shape[0]\n",
    "# Reshape data to separate each element's hourly data\n",
    "data_reshaped = X.reshape(X_data_len, 18, 9)\n",
    "test_data_reshaped = X_test.reshape(X_test_data_len, 18, 9)\n",
    "\n",
    "# Prepare an array to hold the average values for each element and the correlations\n",
    "element_means = np.zeros((18, NUM_TRAINING_DAYS))\n",
    "correlations = np.zeros(18)\n",
    "\n",
    "# Calculate the mean for each element across all hours and correlate with PM2.5\n",
    "for i in range(18):  # For each element\n",
    "    element_means[i] = data_reshaped[:, i, :].mean(axis=1)\n",
    "    correlations[i] = np.corrcoef(element_means[i], y)[0, 1]\n",
    "\n",
    "# Find indices of the top 5 elements with the highest absolute correlation, excluding PM2.5\n",
    "top_elements_indices = np.argsort(-np.abs(correlations))[:NUM_FEATURES]\n",
    "\n",
    "# Extract these elements' data from training and testing datasets\n",
    "selected_features_train = data_reshaped[:, top_elements_indices, :]\n",
    "selected_features_test = test_data_reshaped[:, top_elements_indices, :]\n",
    "\n",
    "# Reshape back to (num_days, num_elements*hours_per_day)\n",
    "selected_features_train = selected_features_train.reshape(X_data_len, len(top_elements_indices)*9)\n",
    "selected_features_test = selected_features_test.reshape(X_test_data_len, len(top_elements_indices)*9)\n",
    "\n",
    "print(\"Selected features shape (training):\", selected_features_train.shape)\n",
    "print(\"Selected features shape (testing):\", selected_features_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10000: RMSE Loss 6.034706771563504\n",
      "epoch 20000: RMSE Loss 5.653954052916235\n",
      "epoch 30000: RMSE Loss 5.486880338192167\n",
      "epoch 40000: RMSE Loss 5.385381198535106\n",
      "epoch 50000: RMSE Loss 5.3098371840956835\n",
      "epoch 60000: RMSE Loss 5.247576502975031\n",
      "epoch 70000: RMSE Loss 5.19381892861529\n",
      "epoch 80000: RMSE Loss 5.146348257716993\n",
      "epoch 90000: RMSE Loss 5.1038966838335265\n",
      "epoch 100000: RMSE Loss 5.065613017865932\n"
     ]
    }
   ],
   "source": [
    "def mse_error(error):\n",
    "    return np.mean(error**2)\n",
    "\n",
    "def normalization(x_data):\n",
    "    maxi = np.max(x_data, axis=0)\n",
    "    mini = np.min(x_data, axis=0)\n",
    "    x_data = (x_data - mini) / (maxi - mini + 1e-10)\n",
    "    return maxi, mini, x_data\n",
    "\n",
    "def gradient_descent(x_data, y_data, length_of_features):\n",
    "    b = 0.0\n",
    "    w = np.ones(length_of_features)\n",
    "    lr = LEARNING_RATE\n",
    "    b_lr = 0.0\n",
    "    w_lr = np.zeros(length_of_features)\n",
    "    lambda_value = 0\n",
    "    \n",
    "    for e in range(N_ITERATIONS):\n",
    "        # y_data = b + w * x_data\n",
    "        error = y_data - b - np.dot(x_data, w) \n",
    "\n",
    "        # Calculate gradient\n",
    "        b_grad = -2 * np.sum(error) * 1\n",
    "        w_grad = -2 * np.dot(error, x_data) + 2 * lambda_value * w\n",
    "        \n",
    "        # Update sum of squares of gradients\n",
    "        b_lr = b_lr + np.square(b_grad)\n",
    "        w_lr = w_lr + np.square(w_grad)\n",
    "\n",
    "        # Update parameters\n",
    "        b = b - lr / np.sqrt(b_lr) * b_grad\n",
    "        w = w - lr / np.sqrt(w_lr) * w_grad\n",
    "        \n",
    "        loss = mse_error(error) + lambda_value * np.sum(np.square(w))\n",
    "        \n",
    "        if (e + 1) % 10000 == 0:\n",
    "            print(f'epoch {e + 1}: RMSE Loss {np.sqrt(loss)}')\n",
    "    return b, w\n",
    "\n",
    "n_features = selected_features_train.shape[1]  \n",
    "maxi, mini, selected_features_train = normalization(selected_features_train)\n",
    "bias, weights = gradient_descent(selected_features_train, y, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240,)\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "selected_features_test = (selected_features_test - mini) / (maxi - mini + 1e-10)\n",
    "test_predictions = selected_features_test.dot(weights) + bias\n",
    "\n",
    "print(test_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "header = ['index', 'answer']\n",
    "filename = 'output.csv'\n",
    "with open(filename, 'w') as file:\n",
    "    csvwriter = csv.writer(file)\n",
    "    csvwriter.writerow(header)\n",
    "    for idx, row in enumerate(test_predictions):\n",
    "        file.write(f\"index_{idx}\" + ', ' + str(row))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
