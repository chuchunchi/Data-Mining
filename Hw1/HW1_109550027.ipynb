{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "N_ITERATIONS = 100000\n",
    "NUM_FEATURES = 7\n",
    "NUM_TRAINING_DAYS = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  ItemName      0      1      2      3      4      5      6  \\\n",
      "0  1/1 00:00  AMB_TEMP  11.10  11.20  11.40  11.50  11.60  11.70  11.90   \n",
      "1  1/1 00:00       CH4   2.01   1.99   2.00   2.02   2.03   2.02   2.02   \n",
      "2  1/1 00:00        CO   0.31   0.28   0.28   0.33   0.32   0.26   0.25   \n",
      "3  1/1 00:00      NMHC   0.10   0.10   0.08   0.09   0.10   0.07   0.07   \n",
      "4  1/1 00:00        NO   1.50   1.40   1.40   1.50   1.40   1.30   1.40   \n",
      "\n",
      "       7  ...     14     15     16     17     18     19     20     21     22  \\\n",
      "0  12.10  ...  16.60  16.30  15.60  14.80  14.40  14.50  14.70  14.70  14.60   \n",
      "1   2.01  ...   1.98   1.97   1.97   2.00   2.02   2.01   2.01   2.00   1.98   \n",
      "2   0.27  ...   0.31   0.29   0.29   0.33   0.34   0.34   0.34   0.29   0.24   \n",
      "3   0.08  ...   0.06   0.07   0.08   0.12   0.13   0.10   0.10   0.09   0.05   \n",
      "4   1.90  ...   3.50   2.60   2.30   2.00   1.80   1.80   1.80   1.70   1.50   \n",
      "\n",
      "      23  \n",
      "0  14.40  \n",
      "1   1.98  \n",
      "2   0.21  \n",
      "3   0.06  \n",
      "4   1.40  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "      Date  ItemName      0      1      2      3      4      5      6      7  \\\n",
      "0  index_0  AMB_TEMP  18.20  17.80  17.50  17.50  17.70  18.10  18.20  18.70   \n",
      "1  index_0       CH4   2.41   2.61   2.65   2.87   2.25   2.24   2.45   2.59   \n",
      "2  index_0        CO   0.77   0.74   0.63   0.60   0.36   0.31   0.48   1.01   \n",
      "3  index_0      NMHC   0.29   0.34   0.34   0.37   0.18   0.15   0.24   0.43   \n",
      "4  index_0        NO   6.80  11.10   9.60  13.60   3.10   2.40  17.80  49.50   \n",
      "\n",
      "       8  \n",
      "0  20.30  \n",
      "1   2.24  \n",
      "2   1.05  \n",
      "3   0.35  \n",
      "4  41.10  \n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_data(filepath, is_testing=False):\n",
    "    # Read the CSV file \n",
    "    if is_testing:\n",
    "        data = pd.read_csv(filepath, header=None)\n",
    "        data.columns = ['Date', 'ItemName', '0', '1', '2', '3', '4', '5', '6', '7', '8']\n",
    "    else:\n",
    "        data = pd.read_csv(filepath)\n",
    "        data.drop(['Location'], axis=1, inplace=True)\n",
    "\n",
    "    # Replace non-numeric values with NaN\n",
    "    non_numeric = {'#': np.nan, '*': np.nan, 'x': np.nan, 'A': np.nan}\n",
    "    data.replace(non_numeric, inplace=True)\n",
    "\n",
    "    # Convert the columns to numeric, except 'Location', 'Date', and 'ItemName'\n",
    "    numeric_columns = data.columns.difference(['Location', 'Date', 'ItemName'])\n",
    "    data[numeric_columns] = data[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Fill NaN values with the median of their respective columns\n",
    "    data[numeric_columns] = data[numeric_columns].fillna(0)\n",
    "\n",
    "    \n",
    "\n",
    "    # Strip the 'ItemName' column\n",
    "    data['ItemName'] = data['ItemName'].str.strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "# Use the function to load and preprocess your data\n",
    "data = load_and_preprocess_data('train.csv')\n",
    "test_data = load_and_preprocess_data('test.csv', is_testing=True)\n",
    "print(data.head())\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (240, 162)\n",
      "Targets shape: (240,)\n",
      "Test Features shape: (240, 162)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_features_targets(dataframe, is_train=True):\n",
    "    # Initialize lists to store features and targets\n",
    "    features_list = []\n",
    "    target_list = []\n",
    "\n",
    "    # Unique dates in the DataFrame\n",
    "    unique_dates = dataframe['Date'].unique()[:NUM_TRAINING_DAYS]\n",
    "\n",
    "    # Loop through each date to extract features and target\n",
    "    for date in unique_dates:\n",
    "        # Filter the DataFrame for the current date\n",
    "        daily_data = dataframe[dataframe['Date'] == date]\n",
    "\n",
    "        # Ensure that the data is sorted by ItemName to maintain consistent feature order\n",
    "        daily_data = daily_data.sort_values('ItemName')\n",
    "\n",
    "        # Extract the feature data for hours 0-9 for all elements\n",
    "        daily_features = daily_data.iloc[:, 2:11].values.flatten()  # Assuming 3rd column is hour 0\n",
    "\n",
    "        # Add the extracted features to the features list\n",
    "        features_list.append(daily_features)\n",
    "\n",
    "        if not is_train:\n",
    "            continue\n",
    "\n",
    "        # Extract the target data (PM2.5 at 10 AM)\n",
    "        pm25_data = daily_data[daily_data['ItemName'] == 'PM2.5']\n",
    "        pm25_at_10am = pm25_data.iloc[0, 11] if not pm25_data.empty else np.nan \n",
    "\n",
    "        # Add the target value to the target list\n",
    "        target_list.append(pm25_at_10am)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    features_array = np.array(features_list)\n",
    "    target_array = np.array(target_list)\n",
    "\n",
    "    return features_array, target_array\n",
    "\n",
    "\n",
    "# Now you can call the function with your DataFrame\n",
    "features, targets = extract_features_targets(data)\n",
    "test_features, _ = extract_features_targets(test_data, is_train=False)\n",
    "\n",
    "# Print shapes to confirm dimensions\n",
    "print('Features shape:', features.shape)\n",
    "print('Targets shape:', targets.shape)\n",
    "print('Test Features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 162) (240,)\n",
      "(240, 162)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(features)\n",
    "X = X.astype(float)\n",
    "y = np.array(targets)\n",
    "y = y.astype(float)\n",
    "\n",
    "X_test = np.array(test_features)\n",
    "X_test = X_test.astype(float)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features shape (training): (240, 63)\n",
      "Selected features shape (testing): (240, 63)\n"
     ]
    }
   ],
   "source": [
    "X_data_len = X.shape[0]\n",
    "X_test_data_len = X_test.shape[0]\n",
    "# Reshape data to separate each element's hourly data\n",
    "data_reshaped = X.reshape(X_data_len, 18, 9)\n",
    "test_data_reshaped = X_test.reshape(X_test_data_len, 18, 9)\n",
    "\n",
    "# Prepare an array to hold the average values for each element and the correlations\n",
    "element_means = np.zeros((18, NUM_TRAINING_DAYS))\n",
    "correlations = np.zeros(18)\n",
    "\n",
    "# Calculate the mean for each element across all hours and correlate with PM2.5\n",
    "for i in range(18):  # For each element\n",
    "    element_means[i] = data_reshaped[:, i, :].mean(axis=1)\n",
    "    correlations[i] = np.corrcoef(element_means[i], y)[0, 1]\n",
    "\n",
    "# Find indices of the top 5 elements with the highest absolute correlation, excluding PM2.5\n",
    "top_elements_indices = np.argsort(-np.abs(correlations))[:NUM_FEATURES]\n",
    "\n",
    "# Extract these elements' data from training and testing datasets\n",
    "selected_features_train = data_reshaped[:, top_elements_indices, :]\n",
    "selected_features_test = test_data_reshaped[:, top_elements_indices, :]\n",
    "\n",
    "# Reshape back to (num_days, num_elements*hours_per_day)\n",
    "selected_features_train = selected_features_train.reshape(X_data_len, len(top_elements_indices)*9)\n",
    "selected_features_test = selected_features_test.reshape(X_test_data_len, len(top_elements_indices)*9)\n",
    "\n",
    "print(\"Selected features shape (training):\", selected_features_train.shape)\n",
    "print(\"Selected features shape (testing):\", selected_features_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.   24.   13.   14.   15.   12.   10.   10.   11.   38.   29.   27.\n",
      " 24.   29.   22.   26.   26.   31.    0.31  0.28  0.28  0.33  0.32  0.26\n",
      "  0.25  0.27  0.32  0.1   0.1   0.08  0.09  0.1   0.07  0.07  0.08  0.1\n",
      " 11.9  10.4   9.8  12.1  12.4   9.2   8.5   9.3  12.4  13.5  11.9  11.2\n",
      " 13.7  13.9  10.6  10.   11.2  15.6   2.11  2.09  2.08  2.11  2.13  2.09\n",
      "  2.09  2.09  2.13]\n"
     ]
    }
   ],
   "source": [
    "print(selected_features_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10000: RMSE Loss 6.034706771563504\n",
      "epoch 20000: RMSE Loss 5.653954052916235\n",
      "epoch 30000: RMSE Loss 5.486880338192167\n",
      "epoch 40000: RMSE Loss 5.385381198535106\n",
      "epoch 50000: RMSE Loss 5.3098371840956835\n",
      "epoch 60000: RMSE Loss 5.247576502975031\n",
      "epoch 70000: RMSE Loss 5.19381892861529\n",
      "epoch 80000: RMSE Loss 5.146348257716993\n",
      "epoch 90000: RMSE Loss 5.1038966838335265\n",
      "epoch 100000: RMSE Loss 5.065613017865932\n"
     ]
    }
   ],
   "source": [
    "def mse_error(error):\n",
    "    return np.mean(error**2)\n",
    "\n",
    "def normalization(x_data):\n",
    "    maxi = np.max(x_data, axis=0)\n",
    "    mini = np.min(x_data, axis=0)\n",
    "    x_data = (x_data - mini) / (maxi - mini + 1e-10)\n",
    "    return maxi, mini, x_data\n",
    "\n",
    "def gradient_descent(x_data, y_data, length_of_features):\n",
    "    b = 0.0\n",
    "    w = np.ones(length_of_features)\n",
    "    lr = LEARNING_RATE\n",
    "    b_lr = 0.0\n",
    "    w_lr = np.zeros(length_of_features)\n",
    "    lambda_value = 0\n",
    "    \n",
    "    for e in range(N_ITERATIONS):\n",
    "        # y_data = b + w * x_data\n",
    "        error = y_data - b - np.dot(x_data, w) \n",
    "\n",
    "        # Calculate gradient\n",
    "        b_grad = -2 * np.sum(error) * 1\n",
    "        w_grad = -2 * np.dot(error, x_data) + 2 * lambda_value * w\n",
    "        \n",
    "        # Update sum of squares of gradients\n",
    "        b_lr = b_lr + np.square(b_grad)\n",
    "        w_lr = w_lr + np.square(w_grad)\n",
    "\n",
    "        # Update parameters\n",
    "        b = b - lr / np.sqrt(b_lr) * b_grad\n",
    "        w = w - lr / np.sqrt(w_lr) * w_grad\n",
    "        \n",
    "        loss = mse_error(error) + lambda_value * np.sum(np.square(w))\n",
    "        \n",
    "        if (e + 1) % 10000 == 0:\n",
    "            print(f'epoch {e + 1}: RMSE Loss {np.sqrt(loss)}')\n",
    "    return b, w\n",
    "\n",
    "n_features = selected_features_train.shape[1]  \n",
    "maxi, mini, selected_features_train = normalization(selected_features_train)\n",
    "bias, weights = gradient_descent(selected_features_train, y, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240,)\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "selected_features_test = (selected_features_test - mini) / (maxi - mini + 1e-10)\n",
    "test_predictions = selected_features_test.dot(weights) + bias\n",
    "\n",
    "print(test_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "header = ['index', 'answer']\n",
    "filename = 'output.csv'\n",
    "with open(filename, 'w') as file:\n",
    "    csvwriter = csv.writer(file)\n",
    "    csvwriter.writerow(header)\n",
    "    for idx, row in enumerate(test_predictions):\n",
    "        file.write(f\"index_{idx}\" + ', ' + str(row))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
